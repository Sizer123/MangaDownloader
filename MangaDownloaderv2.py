import requests
import os
import re
import subprocess
import time
import signal
import sys
from bs4 import BeautifulSoup
from PIL import Image
from urllib.parse import urljoin, urlparse
import logging
import urllib3
import threading
import psutil

# D√©sactive les warnings SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VPNManager:
    def __init__(self, config_file_path, auth_file_path=None):
        """
        Gestionnaire VPN OpenVPN
        
        Args:
            config_file_path (str): Chemin vers le fichier .ovpn
            auth_file_path (str): Chemin vers le fichier d'authentification (optionnel)
        """
        self.config_file = config_file_path
        self.auth_file = auth_file_path
        self.process = None
        self.is_connected = False
        
    def connect(self):
        """D√©marre la connexion VPN"""
        try:
            logger.info("üîí Connexion au VPN...")
            
            # V√©rifie que le fichier de configuration existe
            if not os.path.exists(self.config_file):
                logger.error(f"‚ùå Fichier de configuration VPN non trouv√©: {self.config_file}")
                return False
            
            # Pr√©pare la commande OpenVPN
            cmd = ["openvpn", "--config", self.config_file]
            
            # Ajoute le fichier d'authentification si fourni
            if self.auth_file and os.path.exists(self.auth_file):
                cmd.extend(["--auth-user-pass", self.auth_file])
            
            # D√©marre OpenVPN
            self.process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                universal_newlines=True
            )
            
            # Attendre quelques secondes pour que la connexion s'√©tablisse
            time.sleep(10)
            
            # V√©rifie si la connexion est active
            if self.process.poll() is None:
                self.is_connected = True
                logger.info("‚úÖ VPN connect√© avec succ√®s")
                
                # V√©rifie l'IP publique
                self.check_ip()
                return True
            else:
                logger.error("‚ùå √âchec de la connexion VPN")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la connexion VPN: {e}")
            return False
    
    def disconnect(self):
        """D√©connecte le VPN"""
        try:
            if self.process and self.process.poll() is None:
                logger.info("üîí D√©connexion du VPN...")
                self.process.terminate()
                
                # Attendre la fin du processus
                try:
                    self.process.wait(timeout=10)
                except subprocess.TimeoutExpired:
                    self.process.kill()
                    self.process.wait()
                
                self.is_connected = False
                logger.info("‚úÖ VPN d√©connect√©")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la d√©connexion VPN: {e}")
    
    def check_ip(self):
        """V√©rifie l'IP publique actuelle"""
        try:
            response = requests.get("https://httpbin.org/ip", timeout=10)
            if response.status_code == 200:
                ip_data = response.json()
                logger.info(f"üåê IP publique: {ip_data.get('origin', 'Inconnue')}")
            else:
                logger.warning("‚ö†Ô∏è Impossible de v√©rifier l'IP publique")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur lors de la v√©rification de l'IP: {e}")
    
    def is_vpn_connected(self):
        """V√©rifie si le VPN est toujours connect√©"""
        return self.process and self.process.poll() is None and self.is_connected

class MangaScraper:
    def __init__(self, base_url, delay=1, vpn_manager=None):
        self.base_url = base_url
        self.delay = delay
        self.vpn_manager = vpn_manager
        self.session = requests.Session()
        
        # Configuration pour √©viter les erreurs SSL
        self.session.verify = False
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://phenix-scans.com/',
            # 'User-Agent': "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36",
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Ch-Ua': '"Not/A)Brand";v="8", "Chromium";v="126", "Google Chrome";v="126"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"macOS"',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
        })
    
    def check_connection(self):
        """V√©rifie la connexion internet et VPN"""
        try:
            # Test de connexion simple
            response = requests.get("https://httpbin.org/ip", timeout=10)
            if response.status_code == 200:
                logger.info("‚úÖ Connexion internet OK")
                return True
            else:
                logger.error("‚ùå Probl√®me de connexion internet")
                return False
        except Exception as e:
            logger.error(f"‚ùå Erreur de connexion: {e}")
            return False
    
    def safe_request(self, url, max_retries=3):
        """Effectue une requ√™te avec gestion d'erreurs et retry"""
        for attempt in range(max_retries):
            try:
                # V√©rifie si le VPN est toujours connect√©
                if self.vpn_manager and not self.vpn_manager.is_vpn_connected():
                    logger.warning("‚ö†Ô∏è VPN d√©connect√©, tentative de reconnexion...")
                    if not self.vpn_manager.connect():
                        logger.error("‚ùå Impossible de reconnecter le VPN")
                        return None
                
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                return response
                
            except requests.exceptions.ConnectionError as e:
                logger.warning(f"‚ö†Ô∏è Erreur de connexion (tentative {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(5 * (attempt + 1))  # D√©lai progressif
                    continue
                else:
                    logger.error(f"‚ùå √âchec de la connexion apr√®s {max_retries} tentatives")
                    return None
            except Exception as e:
                logger.error(f"‚ùå Erreur lors de la requ√™te: {e}")
                return None
        
        return None
        
    def get_manga_title(self):
        """R√©cup√®re le titre du manga depuis l'URL principale"""
        try:
            logger.info("R√©cup√©ration du titre du manga... : " + self.base_url)
            response = self.safe_request(self.base_url)
            if not response:
                return "manga_unknown"

            logger.info(f"Statut de la requ√™te: {response.status_code}")
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Essaie diff√©rents s√©lecteurs pour trouver le titre
            title_selectors = [
                'h1.project__content-informations-title'
            ]
            
            for selector in title_selectors:
                title_element = soup.select_one(selector)
                if title_element:
                    title = self.sanitize_filename(title_element.get_text().strip())
                    logger.info(f"Titre trouv√©: {title}")
                    return title
            
            # Si aucun titre trouv√©, utilise l'URL
            title = self.sanitize_filename(self.base_url.split('/')[-1])
            logger.info(f"Titre par d√©faut: {title}")
            return title
            
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration du titre: {e}")
            return "manga_unknown"
    
    def get_chapter_links(self):
        """R√©cup√®re tous les liens des chapitres"""
        try:
            logger.info("R√©cup√©ration des liens des chapitres...")
            response = self.safe_request(self.base_url)
            if not response:
                return []
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            chapter_links = []
            
            # Recherche les liens de chapitres avec diff√©rents s√©lecteurs
            selectors = [
                '.project__chapters a.project__chapter.unstyled-link',
                '.project__chapters a',
                'a.project__chapter',
                '.project__chapter',
                'a[href*="chapter"]',
                '.chapter-list a',
                '.wp-manga-chapter a',
                '.listing-chapters_wrap a',
                '.chapter-link'
            ]
            
            for selector in selectors:
                links = soup.select(selector)
                logger.info(f"S√©lecteur '{selector}': {len(links)} liens trouv√©s")
                if links:
                    for link in links:
                        href = link.get('href')
                        if href and ('chapter' in href.lower() or 'chapitre' in href.lower()):
                            full_url = urljoin(self.base_url, href)
                            chapter_title = self.get_chapter_title(link)
                            chapter_links.append({
                                'url': full_url,
                                'title': chapter_title
                            })
                    if chapter_links:
                        break
            
            # Supprime les doublons
            unique_links = []
            seen_urls = set()
            for chapter in chapter_links:
                if chapter['url'] not in seen_urls:
                    unique_links.append(chapter)
                    seen_urls.add(chapter['url'])
            
            # Trie les chapitres par num√©ro si possible
            chapter_links = self.sort_chapters(unique_links)
            
            logger.info(f"Trouv√© {len(chapter_links)} chapitres uniques")
            
            # Affiche les premiers chapitres pour v√©rification
            if chapter_links:
                logger.info("Premiers chapitres trouv√©s:")
                for i, chapter in enumerate(chapter_links[:5]):
                    logger.info(f"  {i+1}. {chapter['title']} -> {chapter['url']}")
            
            return chapter_links
            
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des liens: {e}")
            return []
    
    def get_chapter_title(self, link_element):
        """Extrait le titre du chapitre depuis l'√©l√©ment du lien"""
        # Essaie diff√©rentes m√©thodes pour extraire le titre
        title = link_element.get_text().strip()
        
        # Si le texte est vide, essaie l'attribut title
        if not title:
            title = link_element.get('title', '').strip()
        
        # Si toujours vide, essaie de construire depuis l'URL
        if not title:
            href = link_element.get('href', '')
            if href:
                # Extrait le dernier segment de l'URL
                segments = href.rstrip('/').split('/')
                title = segments[-1] if segments else 'chapter_unknown'
        
        # Nettoie et retourne le titre
        return self.sanitize_filename(title) if title else 'chapter_unknown'
    
    def sort_chapters(self, chapter_links):
        """Trie les chapitres par num√©ro"""
        def extract_chapter_number(chapter):
            # Recherche un nombre dans le titre ou l'URL
            text_to_search = f"{chapter['title']} {chapter['url']}"
            matches = re.findall(r'(\d+(?:\.\d+)?)', text_to_search)
            
            if matches:
                # Prend le dernier nombre trouv√© (souvent le num√©ro de chapitre)
                return float(matches[-1])
            return 0
        
        sorted_chapters = sorted(chapter_links, key=extract_chapter_number)
        
        # Log du tri pour v√©rification
        logger.info("Ordre des chapitres apr√®s tri:")
        for i, chapter in enumerate(sorted_chapters[:10]):  # Affiche les 10 premiers
            logger.info(f"  {i+1}. {chapter['title']}")
        
        return sorted_chapters
    
    def get_chapter_images(self, chapter_url):
        """R√©cup√®re toutes les images d'un chapitre"""
        try:
            logger.info(f"R√©cup√©ration des images de: {chapter_url}")
            response = self.safe_request(chapter_url)
            if not response:
                return []
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            image_urls = []
            
            # Recherche les images avec diff√©rents s√©lecteurs
            selectors = [
                '.chapter-images img',
                '.reading-content img',
                '.chapter-content img',
                '.manga-reader img',
                '.wp-manga-chapter-img',
                'img[src*="cdn"]',
                'img[data-src]',
                '.page-break img',
                '.separator img',
                'img'  # S√©lecteur g√©n√©rique en dernier recours
            ]
            
            for selector in selectors:
                images = soup.select(selector)
                logger.info(f"S√©lecteur '{selector}': {len(images)} images trouv√©es")
                
                if images:
                    for img in images:
                        src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
                        if src and self.is_valid_image_url(src):
                            full_url = urljoin(chapter_url, src)
                            image_urls.append(full_url)
                    
                    # Si on a trouv√© des images valides, on s'arr√™te
                    if image_urls:
                        break
            
            logger.info(f"Trouv√© {len(image_urls)} images valides pour ce chapitre")
            
            # Affiche les premi√®res images pour v√©rification
            if image_urls:
                logger.info("Premi√®res images trouv√©es:")
                for i, url in enumerate(image_urls[:3]):
                    logger.info(f"  {i+1}. {url}")
            
            return image_urls
            
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des images: {e}")
            return []
    
    def is_valid_image_url(self, url):
        """V√©rifie si l'URL est une image valide"""
        if not url:
            return False
        
        # V√©rifie les extensions d'image
        image_extensions = ['.jpg', '.jpeg', '.png', '.webp', '.gif', '.bmp']
        url_lower = url.lower()
        
        # V√©rifie si l'URL contient une extension d'image
        has_image_ext = any(ext in url_lower for ext in image_extensions)
        
        # √âvite les images de profil, ic√¥nes, etc.
        avoid_keywords = ['avatar', 'profile', 'icon', 'logo', 'button', 'banner']
        has_avoid_keyword = any(keyword in url_lower for keyword in avoid_keywords)
        
        return has_image_ext and not has_avoid_keyword
    
    def download_image(self, url, filepath):
        """T√©l√©charge une image avec gestion d'erreurs"""
        try:
            logger.info(f"T√©l√©chargement: {os.path.basename(filepath)}")
            response = self.safe_request(url)
            if not response:
                return False
            
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            # V√©rifie que le fichier a √©t√© t√©l√©charg√© et n'est pas vide
            if os.path.exists(filepath) and os.path.getsize(filepath) > 0:
                logger.info(f"‚úì Image t√©l√©charg√©e: {os.path.basename(filepath)} ({os.path.getsize(filepath)} bytes)")
                return True
            else:
                logger.error(f"‚úó Fichier vide ou non cr√©√©: {filepath}")
                return False
            
        except Exception as e:
            logger.error(f"Erreur lors du t√©l√©chargement de {url}: {e}")
            return False
    
    def images_to_pdf(self, image_paths, output_pdf):
        """Convertit une liste d'images en PDF"""
        try:
            if not image_paths:
                logger.error("Aucune image √† convertir en PDF")
                return False
            
            logger.info(f"Conversion de {len(image_paths)} images en PDF...")
            
            images = []
            for img_path in image_paths:
                try:
                    img = Image.open(img_path)
                    # Convertit en RGB si n√©cessaire
                    if img.mode != 'RGB':
                        img = img.convert('RGB')
                    images.append(img)
                    logger.info(f"‚úì Image charg√©e: {os.path.basename(img_path)}")
                except Exception as e:
                    logger.error(f"‚úó Erreur lors de l'ouverture de l'image {img_path}: {e}")
                    continue
            
            if images:
                images[0].save(output_pdf, save_all=True, append_images=images[1:])
                logger.info(f"‚úì PDF cr√©√©: {output_pdf}")
                return True
            else:
                logger.error("‚úó Aucune image valide pour cr√©er le PDF")
                return False
            
        except Exception as e:
            logger.error(f"Erreur lors de la cr√©ation du PDF: {e}")
            return False
    
    def sanitize_filename(self, filename):
        """Nettoie le nom de fichier pour √©viter les caract√®res interdits"""
        # Remplace les caract√®res interdits
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        # Supprime les espaces en d√©but/fin
        filename = filename.strip()
        # Remplace les espaces multiples par un seul
        filename = re.sub(r'\s+', ' ', filename)
        # Limite la longueur
        if len(filename) > 100:
            filename = filename[:100]
        return filename
    
    def scrape_manga(self):
        """Fonction principale pour scraper tout le manga"""
        logger.info("üöÄ D√©but du scraping...")
        
        # V√©rifie la connexion
        if not self.check_connection():
            logger.error("‚ùå Probl√®me de connexion, arr√™t du scraping")
            return
        
        # R√©cup√®re le titre du manga
        manga_title = self.get_manga_title()
        logger.info(f"üìñ Titre du manga: {manga_title}")
        
        # Cr√©e le dossier principal
        main_folder = os.path.join("manga_downloads", manga_title)
        os.makedirs(main_folder, exist_ok=True)
        logger.info(f"üìÅ Dossier cr√©√©: {main_folder}")
        
        # R√©cup√®re les liens des chapitres
        chapter_links = self.get_chapter_links()
        if not chapter_links:
            logger.error("‚ùå Aucun chapitre trouv√©")
            return
        
        logger.info(f"üìö {len(chapter_links)} chapitres √† t√©l√©charger")
        
        # Traite chaque chapitre
        successful_chapters = 0
        for i, chapter in enumerate(chapter_links, 1):
            logger.info(f"\nüìÑ Traitement du chapitre {i}/{len(chapter_links)}: {chapter['title']}")
            
            # Cr√©e un dossier temporaire pour les images
            temp_folder = os.path.join(main_folder, f"temp_{chapter['title']}")
            os.makedirs(temp_folder, exist_ok=True)
            
            try:
                # R√©cup√®re les images du chapitre
                image_urls = self.get_chapter_images(chapter['url'])
                if not image_urls:
                    logger.warning(f"‚ö†Ô∏è Aucune image trouv√©e pour {chapter['title']}")
                    continue
                
                # T√©l√©charge les images
                downloaded_images = []
                for j, img_url in enumerate(image_urls):
                    img_extension = self.get_image_extension(img_url)
                    img_filename = f"page_{j+1:03d}{img_extension}"
                    img_path = os.path.join(temp_folder, img_filename)
                    
                    if self.download_image(img_url, img_path):
                        downloaded_images.append(img_path)
                    
                    # D√©lai entre les t√©l√©chargements
                    time.sleep(self.delay)
                
                # Convertit en PDF
                if downloaded_images:
                    pdf_filename = f"{chapter['title']}.pdf"
                    pdf_path = os.path.join(main_folder, pdf_filename)
                    
                    if self.images_to_pdf(downloaded_images, pdf_path):
                        logger.info(f"‚úÖ Chapitre {chapter['title']} termin√© avec succ√®s")
                        successful_chapters += 1
                    else:
                        logger.error(f"‚ùå √âchec de la cr√©ation du PDF pour {chapter['title']}")
                else:
                    logger.warning(f"‚ö†Ô∏è Aucune image t√©l√©charg√©e pour {chapter['title']}")
                
                # Nettoie le dossier temporaire
                for img_path in downloaded_images:
                    try:
                        os.remove(img_path)
                    except:
                        pass
                
                # Supprime le dossier temporaire s'il est vide
                try:
                    os.rmdir(temp_folder)
                except:
                    pass
                
            except Exception as e:
                logger.error(f"‚ùå Erreur lors du traitement du chapitre {chapter['title']}: {e}")
                continue
            
            # D√©lai entre les chapitres
            time.sleep(self.delay * 2)
        
        logger.info(f"\nüéâ Scraping termin√©! {successful_chapters}/{len(chapter_links)} chapitres t√©l√©charg√©s avec succ√®s")
    
    def get_image_extension(self, url):
        """Extrait l'extension d'image de l'URL"""
        extensions = ['.jpg', '.jpeg', '.png', '.webp', '.gif', '.bmp']
        url_lower = url.lower()
        
        for ext in extensions:
            if ext in url_lower:
                return ext
        
        return '.jpg'  # Extension par d√©faut

def signal_handler(sig, frame):
    """Gestionnaire pour l'arr√™t propre du programme"""
    logger.info("\n‚ö†Ô∏è Arr√™t du programme demand√©...")
    if 'vpn' in globals() and vpn.is_connected:
        vpn.disconnect()
    sys.exit(0)

def main():
    """Fonction principale"""
    # Gestionnaire de signal pour arr√™t propre
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # Configuration VPN
    vpn_auth_file = "/Users/orangedigitalcenter/Documents/Projects/Manhwa/auth.txt"  # Remplacez par votre fichier .ovpn
    vpn_config_file = "/Users/orangedigitalcenter/Documents/Projects/Manhwa/us-free-8.protonvpn.tcp.ovpn"       # Optionnel: fichier avec username/password
    
    # URL du manga
    manga_url = "https://phenix-scans.com/manga/a-modern-man-who-transmigrated-into-the-murim-world"
    
    # Initialise le gestionnaire VPN
    vpn = VPNManager(vpn_config_file, vpn_auth_file)
    
    try:
        # Connecte le VPN
        if vpn.connect():
            logger.info("‚úÖ VPN connect√©, d√©but du scraping...")
            
            # Cr√©e le scraper avec le gestionnaire VPN
            scraper = MangaScraper(manga_url, delay=1, vpn_manager=vpn)
            
            # Lance le scraping
            scraper.scrape_manga()
        else:
            logger.error("‚ùå Impossible de connecter le VPN")
            
            # Demande √† l'utilisateur s'il veut continuer sans VPN
            response = input("Voulez-vous continuer sans VPN? (y/N): ")
            if response.lower() == 'y':
                scraper = MangaScraper(manga_url, delay=1)
                scraper.scrape_manga()
            else:
                logger.info("Arr√™t du programme")
                return
    
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è Arr√™t du programme par l'utilisateur")
    
    except Exception as e:
        logger.error(f"‚ùå Erreur inattendue: {e}")
    
    finally:
        # D√©connecte le VPN proprement
        if vpn.is_connected:
            vpn.disconnect()
        logger.info("üèÅ Programme termin√©")

if __name__ == "__main__":
    main()