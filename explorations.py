import requests
import os
import time
import random
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import logging
from fake_useragent import UserAgent

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class MangaDownloader:
    def __init__(self):
        self.session = requests.Session()
        self.ua = UserAgent()
        self.setup_session()
        
    def setup_session(self):
        """Configure la session avec headers et cookies pour √©viter les blocages"""
        # Headers r√©alistes
        headers = {
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
        }
        self.session.headers.update(headers)
        
        # D√©sactiver la v√©rification SSL si n√©cessaire
        self.session.verify = False
        
        # Timeout plus long
        self.session.timeout = 30
        
    def rotate_user_agent(self):
        """Change l'User-Agent pour √©viter la d√©tection"""
        self.session.headers['User-Agent'] = self.ua.random
        
    def get_page_with_retry(self, url, max_retries=3):
        """R√©cup√®re une page avec retry et gestion des erreurs"""
        for attempt in range(max_retries):
            try:
                # Rotation de l'User-Agent
                self.rotate_user_agent()
                
                # D√©lai al√©atoire entre les requ√™tes
                if attempt > 0:
                    delay = random.uniform(2, 5)
                    logger.info(f"Tentative {attempt + 1}/{max_retries} - Attente de {delay:.1f}s...")
                    time.sleep(delay)
                
                response = self.session.get(url)
                
                if response.status_code == 200:
                    return response
                elif response.status_code == 403:
                    logger.warning(f"Erreur 403 - Tentative {attempt + 1}/{max_retries}")
                    if attempt < max_retries - 1:
                        # Essayer avec des headers diff√©rents
                        self.try_different_headers()
                        continue
                else:
                    logger.warning(f"Code de statut {response.status_code} - Tentative {attempt + 1}/{max_retries}")
                    
            except requests.exceptions.RequestException as e:
                logger.error(f"Erreur de requ√™te: {e}")
                if attempt < max_retries - 1:
                    time.sleep(random.uniform(3, 6))
                    continue
                    
        return None
    
    def try_different_headers(self):
        """Essaie diff√©rents headers pour contourner les blocages"""
        headers_options = [
            {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Referer': 'https://www.google.com/',
            },
            {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Referer': 'https://phenix-scans.com/',
            },
            {
                'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            }
        ]
        
        selected_headers = random.choice(headers_options)
        self.session.headers.update(selected_headers)
        logger.info("Headers mis √† jour pour contourner les blocages")
    
    def get_manga_title(self, url):
        """R√©cup√®re le titre du manga"""
        logger.info(f"R√©cup√©ration du titre du manga... : {url}")
        
        response = self.get_page_with_retry(url)
        if not response:
            logger.error("Impossible de r√©cup√©rer la page du manga")
            return "manga_unknown"
        
        try:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Diff√©rents s√©lecteurs possibles pour le titre
            title_selectors = [
                'h1.entry-title',
                'h1.manga-title',
                'h1.post-title',
                'h1',
                '.manga-title',
                '.entry-title',
                '.post-title'
            ]
            
            for selector in title_selectors:
                title_element = soup.select_one(selector)
                if title_element:
                    title = title_element.get_text(strip=True)
                    logger.info(f"üìñ Titre trouv√©: {title}")
                    return self.sanitize_filename(title)
            
            logger.warning("Titre non trouv√©, utilisation du nom par d√©faut")
            return "manga_unknown"
            
        except Exception as e:
            logger.error(f"Erreur lors de l'extraction du titre: {e}")
            return "manga_unknown"
    
    def get_chapter_links(self, url):
        """R√©cup√®re les liens des chapitres"""
        logger.info("R√©cup√©ration des liens des chapitres...")
        
        response = self.get_page_with_retry(url)
        if not response:
            logger.error("Impossible de r√©cup√©rer la page des chapitres")
            return []
        
        try:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Diff√©rents s√©lecteurs pour les liens de chapitres
            chapter_selectors = [
                '.wp-manga-chapter a',
                '.chapter-link',
                '.listing-chapters_wrap a',
                'a[href*="chapter"]',
                '.chapter a',
                'li a[href*="chapter"]'
            ]
            
            chapters = []
            for selector in chapter_selectors:
                chapter_elements = soup.select(selector)
                if chapter_elements:
                    logger.info(f"Chapitres trouv√©s avec le s√©lecteur: {selector}")
                    for element in chapter_elements:
                        href = element.get('href')
                        if href:
                            full_url = urljoin(url, href)
                            chapter_name = element.get_text(strip=True)
                            chapters.append({
                                'name': chapter_name,
                                'url': full_url
                            })
                    break
            
            if chapters:
                logger.info(f"üìö {len(chapters)} chapitres trouv√©s")
                return chapters[::-1]  # Inverser pour commencer par le chapitre 1
            else:
                logger.error("Aucun chapitre trouv√© avec les s√©lecteurs disponibles")
                return []
                
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des chapitres: {e}")
            return []
    
    def get_chapter_images(self, chapter_url):
        """R√©cup√®re les images d'un chapitre"""
        logger.info(f"R√©cup√©ration des images du chapitre: {chapter_url}")
        
        response = self.get_page_with_retry(chapter_url)
        if not response:
            logger.error("Impossible de r√©cup√©rer la page du chapitre")
            return []
        
        try:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Diff√©rents s√©lecteurs pour les images
            image_selectors = [
                '.reading-content img',
                '.chapter-content img',
                '.page-break img',
                'img[src*="uploads"]',
                '.wp-manga-chapter-img img',
                'img'
            ]
            
            images = []
            for selector in image_selectors:
                img_elements = soup.select(selector)
                if img_elements:
                    for img in img_elements:
                        src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
                        if src and any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                            full_url = urljoin(chapter_url, src)
                            images.append(full_url)
                    if images:
                        break
            
            logger.info(f"üì∑ {len(images)} images trouv√©es")
            return images
            
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des images: {e}")
            return []
    
    def download_image(self, img_url, file_path):
        """T√©l√©charge une image"""
        try:
            response = self.get_page_with_retry(img_url)
            if response:
                with open(file_path, 'wb') as f:
                    f.write(response.content)
                return True
        except Exception as e:
            logger.error(f"Erreur lors du t√©l√©chargement de l'image: {e}")
        return False
    
    def sanitize_filename(self, filename):
        """Nettoie le nom de fichier"""
        invalid_chars = '<>:"/\\|?*'
        for char in invalid_chars:
            filename = filename.replace(char, '_')
        return filename.strip()
    
    def download_manga(self, manga_url, max_chapters=None):
        """T√©l√©charge le manga complet"""
        logger.info("üöÄ D√©but du scraping...")
        
        # R√©cup√©ration du titre
        manga_title = self.get_manga_title(manga_url)
        
        # Cr√©ation du dossier
        download_dir = os.path.join('manga_downloads', manga_title)
        os.makedirs(download_dir, exist_ok=True)
        logger.info(f"üìÅ Dossier cr√©√©: {download_dir}")
        
        # R√©cup√©ration des chapitres
        chapters = self.get_chapter_links(manga_url)
        
        if not chapters:
            logger.error("‚ùå Aucun chapitre trouv√©")
            return
        
        # Limitation du nombre de chapitres si sp√©cifi√©
        if max_chapters:
            chapters = chapters[:max_chapters]
        
        # T√©l√©chargement des chapitres
        for i, chapter in enumerate(chapters, 1):
            logger.info(f"üìñ T√©l√©chargement du chapitre {i}/{len(chapters)}: {chapter['name']}")
            
            # Cr√©ation du dossier du chapitre
            chapter_dir = os.path.join(download_dir, self.sanitize_filename(chapter['name']))
            os.makedirs(chapter_dir, exist_ok=True)
            
            # R√©cup√©ration des images
            images = self.get_chapter_images(chapter['url'])
            
            if not images:
                logger.warning(f"Aucune image trouv√©e pour le chapitre: {chapter['name']}")
                continue
            
            # T√©l√©chargement des images
            for j, img_url in enumerate(images, 1):
                img_extension = os.path.splitext(urlparse(img_url).path)[1] or '.jpg'
                img_filename = f"page_{j:03d}{img_extension}"
                img_path = os.path.join(chapter_dir, img_filename)
                
                if not os.path.exists(img_path):
                    logger.info(f"  üì∑ T√©l√©chargement de l'image {j}/{len(images)}")
                    if self.download_image(img_url, img_path):
                        logger.info(f"    ‚úÖ Image sauvegard√©e: {img_filename}")
                    else:
                        logger.error(f"    ‚ùå √âchec du t√©l√©chargement: {img_filename}")
                else:
                    logger.info(f"  ‚è≠Ô∏è  Image d√©j√† t√©l√©charg√©e: {img_filename}")
                
                # D√©lai entre les t√©l√©chargements
                time.sleep(random.uniform(0.5, 1.5))
            
            # D√©lai entre les chapitres
            time.sleep(random.uniform(2, 4))
        
        logger.info("üéâ T√©l√©chargement termin√©!")

def main():
    # URL du manga √† t√©l√©charger
    manga_url = "https://phenix-scans.com/manga/a-modern-man-who-transmigrated-into-the-murim-world"
    
    # Cr√©ation du downloader
    downloader = MangaDownloader()
    
    # T√©l√©chargement (limit√© √† 3 chapitres pour les tests)
    downloader.download_manga(manga_url, max_chapters=3)

if __name__ == "__main__":
    main()